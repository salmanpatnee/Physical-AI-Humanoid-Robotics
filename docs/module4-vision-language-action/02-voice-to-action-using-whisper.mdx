---
title: "2. Voice-to-Action using Whisper"
sidebar_position: 2
chapter_type: "tutorial"
learning_goals:
  - "Deploy OpenAI Whisper for speech-to-text conversion"
  - "Integrate voice commands with ROS 2 robot control"
  - "Handle real-time audio processing for robot interaction"
prerequisites:
  - "Python programming experience"
  - "ROS 2 basics from Module 1"
  - "Microphone hardware access"
key_takeaways:
  - "Whisper enables multilingual voice control for robots"
  - "Real-time ASR requires efficient audio buffering and processing"
  - "Voice interfaces make robots accessible to non-technical users"
---

# Voice-to-Action using Whisper

<WhatYouWillLearn
  goals={[
    {
      text: "Deploy OpenAI Whisper for speech-to-text conversion",
      icon: "Target",
      why_it_matters: "This is important for building real robots!"
    },
    {
      text: "Integrate voice commands with ROS 2 robot control",
      icon: "Lightbulb",
      why_it_matters: "This is important for building real robots!"
    },
    {
      text: "Handle real-time audio processing for robot interaction",
      icon: "Rocket",
      why_it_matters: "This is important for building real robots!"
    }
  ]}
  displayStyle="cards"
/>


## Introduction: Enabling Voice Control

The first step in a Vision-Language-Action system is capturing human intent through speech. **OpenAI Whisper** is a state-of-the-art automatic speech recognition (ASR) model that converts spoken commands into text with near-human accuracy across 99 languages.

<LearningGoals>
- Deploy OpenAI Whisper for speech-to-text conversion
- Integrate voice commands with ROS 2 robot control
- Handle real-time audio processing for robot interaction
</LearningGoals>

<Prerequisites>
- Python programming experience
- ROS 2 basics from Module 1
- Microphone hardware access
</Prerequisites>

## What is OpenAI Whisper?

Whisper is a transformer-based neural network trained on 680,000 hours of multilingual audio. It offers several model sizes:

| Model | Parameters | Speed | Use Case |
|-------|-----------|-------|----------|
| tiny | 39M | ~32x realtime | Edge devices, quick prototyping |
| base | 74M | ~16x realtime | Balanced performance |
| small | 244M | ~6x realtime | **Recommended for robotics** |
| medium | 769M | ~2x realtime | High accuracy needs |
| large | 1550M | ~1x realtime | Maximum accuracy |

For robot applications, **small** provides the best speed-accuracy tradeoff.

## Installing Whisper

```bash
# Install Whisper
pip install -U openai-whisper

# Install audio dependencies
sudo apt-get install ffmpeg portaudio19-dev
pip install pyaudio soundfile
```

## Basic Speech-to-Text

```python
import whisper

# Load model (downloads once, then cached)
model = whisper.load_model("small")

# Transcribe audio file
result = model.transcribe("command.wav")
print(result["text"])
# Output: "Move the robot forward two meters"
```

## Building a Voice-Controlled Robot

### ROS 2 Voice Command Node

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import whisper
import pyaudio
import numpy as np
import threading

class VoiceCommandNode(Node):
    def __init__(self):
        super().__init__('voice_command')

        self.publisher = self.create_publisher(String, 'voice_commands', 10)

        # Load Whisper
        self.get_logger().info("Loading Whisper model...")
        self.model = whisper.load_model("small")

        # Audio settings
        self.RATE = 16000  # Whisper expects 16kHz
        self.CHUNK = 1024
        self.record_seconds = 3

        # Start listening thread
        threading.Thread(target=self.listen_loop, daemon=True).start()

    def record_audio(self):
        """Capture audio from microphone"""
        p = pyaudio.PyAudio()
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK
        )

        self.get_logger().info("ðŸŽ¤ Listening...")

        frames = []
        for _ in range(0, int(self.RATE / self.CHUNK * self.record_seconds)):
            data = stream.read(self.CHUNK)
            frames.append(np.frombuffer(data, dtype=np.int16))

        stream.stop_stream()
        stream.close()
        p.terminate()

        # Convert to float32 normalized audio
        audio = np.concatenate(frames).astype(np.float32) / 32768.0
        return audio

    def listen_loop(self):
        """Continuous listening"""
        while rclpy.ok():
            audio = self.record_audio()

            # Transcribe
            result = self.model.transcribe(audio, language='en', fp16=False)
            text = result["text"].strip()

            if text:
                self.get_logger().info(f"Heard: '{text}'")

                msg = String()
                msg.data = text
                self.publisher.publish(msg)

def main():
    rclpy.init()
    node = VoiceCommandNode()
    rclpy.spin(node)
```

### Command Parser and Executor

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist
import re

class CommandExecutor(Node):
    def __init__(self):
        super().__init__('command_executor')

        self.subscription = self.create_subscription(
            String, 'voice_commands', self.execute_command, 10)

        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)

    def execute_command(self, msg):
        command = msg.data.lower()

        # Simple pattern matching (Chapter 3 will use LLM)
        if 'forward' in command or 'ahead' in command:
            self.move(0.5, 0.0)
        elif 'backward' in command or 'back' in command:
            self.move(-0.3, 0.0)
        elif 'left' in command:
            self.move(0.0, 0.5)
        elif 'right' in command:
            self.move(0.0, -0.5)
        elif 'stop' in command or 'halt' in command:
            self.move(0.0, 0.0)
        else:
            self.get_logger().warn(f"Unknown command: {command}")

    def move(self, linear, angular):
        twist = Twist()
        twist.linear.x = linear
        twist.angular.z = angular
        self.cmd_pub.publish(twist)
        self.get_logger().info(f"Velocity: linear={linear}, angular={angular}")

def main():
    rclpy.init()
    node = CommandExecutor()
    rclpy.spin(node)
```

## Handling Challenges

### 1. Background Noise

```python
# Use voice activity detection (VAD)
import webrtcvad

vad = webrtcvad.Vad(2)  # Aggressiveness 0-3

# Only transcribe if speech detected
if vad.is_speech(audio_frame, sample_rate=16000):
    result = model.transcribe(audio)
```

### 2. Multi-Language Support

```python
# Auto-detect language
result = model.transcribe(audio, task='transcribe')
detected_lang = result['language']

# Or specify language
result = model.transcribe(audio, language='es')  # Spanish
```

### 3. Continuous Listening

```python
# Use push-to-talk button
import keyboard

if keyboard.is_pressed('space'):
    audio = record_audio()
    transcribe(audio)
```

<KeyTakeaways>
- Whisper enables multilingual voice control for robots
- Real-time ASR requires efficient audio buffering and processing
- Voice interfaces make robots accessible to non-technical users
</KeyTakeaways>

## Practical Exercise

<ExerciseBlock
  question="Extend the voice command system to handle commands with parameters, such as 'Move forward 2 meters' or 'Turn left 90 degrees'. Extract the numeric values and convert them to appropriate velocity commands."
  hints={[
    { title: "Hint 1", content: "Use regular expressions to extract numbers from the transcribed text." },
    { title: "Hint 2", content: "Define a mapping from units (meters, degrees) to robot velocity commands." },
    { title: "Hint 3", content: "Calculate duration based on desired distance and robot's maximum velocity." }
  ]}
  solution={
    <div>
      <p><strong>Solution:</strong></p>
      <pre>{`
import re

def parse_command(text):
    text = text.lower()

    # Extract numbers
    numbers = re.findall(r'\\d+\\.?\\d*', text)

    if 'forward' in text and numbers:
        distance = float(numbers[0])
        # Assume max speed 0.5 m/s
        duration = distance / 0.5
        return ('move_forward', distance, duration)

    elif 'turn' in text and numbers:
        angle = float(numbers[0])
        # Assume max rotation 30 deg/s
        duration = angle / 30.0
        direction = 'left' if 'left' in text else 'right'
        return ('turn', angle, duration, direction)

    return None

# Usage
result = parse_command("Move forward 2 meters")
# Returns: ('move_forward', 2.0, 4.0)
      `}</pre>
    </div>
  }
/>


## Technical Terms Glossary

<GrownUpWords
  terms={[
    {
      simple_term: "Robot's language",
      technical_term: "ROS 2",
      context_example: "This chapter discusses ROS 2 in detail."
    },
    {
      simple_term: "Robot part or worker",
      technical_term: "Node",
      context_example: "This chapter discusses Node in detail."
    },
    {
      simple_term: "Smart language helper",
      technical_term: "LLM",
      context_example: "This chapter discusses LLM in detail."
    },
    {
      simple_term: "Learning pattern matcher",
      technical_term: "Neural Network",
      context_example: "This chapter discusses Neural Network in detail."
    }
  ]}
  displayStyle="table"
/>

## Next Steps

In Chapter 3, we'll replace simple pattern matching with **LLM-based cognitive planning**, enabling understanding of complex multi-step commands like "Go to the kitchen and bring me a glass of water."
