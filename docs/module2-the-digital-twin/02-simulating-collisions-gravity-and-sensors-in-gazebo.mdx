---
title: "2. Simulating Collisions, Gravity, and Sensors in Gazebo"
sidebar_position: 2
chapter_type: "hands-on"
learning_goals:
  - "Configure collision properties for realistic object interactions"
  - "Understand and modify gravity settings for different environments"
  - "Add and configure sensors in Gazebo robot models"
  - "Visualize sensor data in real-time"
prerequisites:
  - "Completion of Chapter 1: Physics Simulation and World Building"
  - "Basic understanding of URDF/SDF formats"
  - "ROS 2 installation with Gazebo"
key_takeaways:
  - "Collision modeling involves defining geometry, friction, and restitution"
  - "Gravity can be customized per-world for different planetary simulations"
  - "Gazebo sensors publish data to ROS 2 topics for processing"
  - "Sensor plugins bridge Gazebo physics with ROS 2 communication"
---

# Simulating Collisions, Gravity, and Sensors in Gazebo

## Collision Modeling

Accurate collision detection is critical for realistic simulation. When a humanoid robot walks, its feet must collide realistically with the ground. When it picks up an object, the gripper must interact properly with the object's surface.

<LearningGoals>
- Configure collision properties for realistic object interactions
- Understand and modify gravity settings for different environments
- Add and configure sensors in Gazebo robot models
- Visualize sensor data in real-time
</LearningGoals>

<Prerequisites>
- Completion of Chapter 1: Physics Simulation and World Building
- Basic understanding of URDF/SDF formats
- ROS 2 installation with Gazebo
</Prerequisites>

### Collision Geometry

Each link in a Gazebo model has two geometries:

1. **Collision geometry**: Simple shapes used for physics calculations
2. **Visual geometry**: Detailed meshes for rendering

**Why separate them?** Physics engines are computationally expensive. Using simple collision shapes (boxes, spheres, cylinders) instead of complex meshes makes simulation run faster while maintaining accuracy.

### Defining Collision Properties

In SDF format:

```xml
<link name="my_link">
  <collision name="collision">
    <geometry>
      <box><size>1 0.5 0.3</size></box>
    </geometry>
    <surface>
      <friction>
        <ode>
          <mu>1.0</mu>      <!-- Coefficient of friction -->
          <mu2>1.0</mu2>    <!-- Secondary friction coefficient -->
        </ode>
      </friction>
      <contact>
        <ode>
          <kp>1e6</kp>      <!-- Contact stiffness -->
          <kd>1.0</kd>      <!-- Contact damping -->
        </ode>
      </contact>
      <bounce>
        <restitution_coefficient>0.0</restitution_coefficient>
      </bounce>
    </surface>
  </collision>
</link>
```

### Key Collision Parameters

**1. Friction Coefficients (`mu`, `mu2`)**

- **mu**: Friction in the primary direction
- **mu2**: Friction in the secondary direction (for anisotropic surfaces like ice or carpet)

Common values:
- Wood on wood: 0.4
- Rubber on concrete: 0.7-1.0
- Ice on ice: 0.05-0.15
- Metal on metal: 0.15-0.3

**2. Contact Stiffness (`kp`) and Damping (`kd`)**

- **Stiffness**: How much force is generated when objects overlap
- **Damping**: How much energy is dissipated during contact

Higher stiffness = harder surface
Higher damping = less bouncing

**3. Restitution Coefficient (Bounciness)**

- 0.0 = No bounce (inelastic collision)
- 1.0 = Perfect bounce (elastic collision)

Common values:
- Basketball: 0.8
- Tennis ball: 0.75
- Humanoid foot on ground: 0.0-0.1

### Example: Configuring a Robot Foot

For a humanoid robot foot, we want:
- High friction (no slipping)
- No bounce (stable stance)
- Moderate stiffness (realistic contact)

```xml
<link name="left_foot">
  <collision name="foot_collision">
    <geometry>
      <box><size>0.2 0.1 0.05</size></box>
    </geometry>
    <surface>
      <friction>
        <ode>
          <mu>1.2</mu>      <!-- High friction -->
          <mu2>1.2</mu2>
        </ode>
      </friction>
      <contact>
        <ode>
          <kp>1e7</kp>      <!-- Stiff contact -->
          <kd>100.0</kd>
          <max_vel>0.01</max_vel>
          <min_depth>0.001</min_depth>
        </ode>
      </contact>
      <bounce>
        <restitution_coefficient>0.0</restitution_coefficient>
      </bounce>
    </surface>
  </collision>
</link>
```

## Gravity Configuration

Gravity is defined in the world's `<physics>` block:

```xml
<physics type="ode">
  <gravity>0 0 -9.81</gravity>  <!-- Earth gravity -->
  <max_step_size>0.001</max_step_size>
  <real_time_factor>1.0</real_time_factor>
  <real_time_update_rate>1000</real_time_update_rate>
</physics>
```

### Understanding Physics Parameters

**1. Gravity Vector**

The gravity vector specifies the direction and magnitude:
- `<gravity>x y z</gravity>`

Examples:
- Earth: `0 0 -9.81`
- Moon: `0 0 -1.62`
- Mars: `0 0 -3.71`
- Zero gravity (space station): `0 0 0`

**2. Simulation Step Size (`max_step_size`)**

The time interval for each physics update. Smaller = more accurate but slower.

Common values:
- Standard: 0.001 (1 ms)
- High precision: 0.0001 (0.1 ms)
- Fast simulation: 0.01 (10 ms)

**3. Real-Time Factor**

- 1.0 = Simulation runs at real-world speed
- 2.0 = Simulation runs 2x faster than reality
- 0.5 = Simulation runs in slow motion

**4. Update Rate**

How many times per second to update physics (Hz).

- For `max_step_size = 0.001`, use `real_time_update_rate = 1000`
- This ensures `1000 steps/sec × 0.001 sec/step = 1.0` real-time factor

### Multi-Gravity Scenario Example

Create a world with multiple gravity zones:

```xml
<world name="multi_gravity">
  <!-- Default Earth gravity -->
  <physics type="ode">
    <gravity>0 0 -9.81</gravity>
  </physics>

  <!-- You can create zones with different gravity using model-specific gravity (advanced feature) -->
  <!-- For this example, we'll demonstrate changing world gravity dynamically in exercises -->
</world>
```

## Introduction to Sensors

Sensors are how robots perceive the world. In simulation, sensor plugins generate synthetic data that mimics real sensor outputs.

### Types of Sensors in Gazebo

| Sensor Type | Measures | Output | Use Case |
|-------------|----------|--------|----------|
| **Camera** | RGB images | Image topic | Object detection, visual servoing |
| **Depth Camera** | RGB + depth | Image + point cloud | 3D perception, SLAM |
| **LiDAR** | Distance via laser | Point cloud | Mapping, obstacle avoidance |
| **IMU** | Acceleration, rotation | Linear/angular velocity | Balance, state estimation |
| **Contact** | Touch/pressure | Boolean + force | Gripper feedback, foot contact |
| **GPS** | Position | Latitude/longitude | Outdoor navigation |
| **Ray** | Single distance | Range | Simple proximity detection |

### Sensor Lifecycle

1. **Definition**: Add sensor to URDF/SDF
2. **Plugin**: Attach ROS 2 plugin to publish data
3. **Simulation**: Gazebo generates synthetic sensor data
4. **Publishing**: Data is published to ROS 2 topics
5. **Processing**: Your nodes subscribe and process the data

## Gazebo Sensors: Cameras

Let's add a camera to a robot model.

### Camera Sensor Definition

In URDF:

```xml
<link name="camera_link">
  <!-- Visual geometry (optional) -->
  <visual>
    <geometry>
      <box size="0.05 0.05 0.05"/>
    </geometry>
  </visual>

  <!-- Sensor definition -->
  <sensor name="camera_sensor" type="camera">
    <update_rate>30.0</update_rate>
    <camera>
      <horizontal_fov>1.047</horizontal_fov>  <!-- 60 degrees -->
      <image>
        <width>640</width>
        <height>480</height>
        <format>R8G8B8</format>
      </image>
      <clip>
        <near>0.1</near>
        <far>100.0</far>
      </clip>
    </camera>

    <!-- ROS 2 plugin to publish camera data -->
    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
      <ros>
        <namespace>/robot</namespace>
        <remapping>image_raw:=camera/image_raw</remapping>
        <remapping>camera_info:=camera/camera_info</remapping>
      </ros>
      <camera_name>front_camera</camera_name>
      <frame_name>camera_link</frame_name>
    </plugin>
  </sensor>
</link>
```

### Camera Parameters Explained

- **update_rate**: Frames per second (Hz)
- **horizontal_fov**: Field of view in radians (1.047 rad ≈ 60°)
- **image/width, height**: Resolution in pixels
- **format**: Color format (R8G8B8 = 8-bit RGB)
- **clip/near, far**: Minimum and maximum rendering distance

### Viewing Camera Output

After launching Gazebo with your robot:

```bash
# List camera topics
ros2 topic list | grep camera

# View image stream with rqt
ros2 run rqt_image_view rqt_image_view

# Echo camera info
ros2 topic echo /robot/camera/camera_info
```

## Adding an IMU Sensor

An **Inertial Measurement Unit (IMU)** measures linear acceleration and angular velocity. Critical for humanoid balance.

### IMU Sensor Definition

```xml
<link name="imu_link">
  <sensor name="imu_sensor" type="imu">
    <always_on>true</always_on>
    <update_rate>100.0</update_rate>
    <imu>
      <angular_velocity>
        <x>
          <noise type="gaussian">
            <mean>0.0</mean>
            <stddev>0.0002</stddev>
          </noise>
        </x>
        <!-- Similar for y and z -->
      </angular_velocity>
      <linear_acceleration>
        <x>
          <noise type="gaussian">
            <mean>0.0</mean>
            <stddev>0.017</stddev>
          </noise>
        </x>
        <!-- Similar for y and z -->
      </linear_acceleration>
    </imu>

    <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">
      <ros>
        <namespace>/robot</namespace>
        <remapping>~/out:=imu/data</remapping>
      </ros>
      <frame_name>imu_link</frame_name>
    </plugin>
  </sensor>
</link>
```

### IMU Output

The IMU publishes a `sensor_msgs/Imu` message containing:

- **orientation**: Quaternion (requires magnetometer or sensor fusion)
- **angular_velocity**: Rotation rate in rad/s (x, y, z)
- **linear_acceleration**: Acceleration in m/s² (x, y, z)

### Visualizing IMU Data

```bash
# View IMU messages
ros2 topic echo /robot/imu/data

# Plot angular velocity over time
ros2 run rqt_plot rqt_plot /robot/imu/data/angular_velocity/x:y:z
```

## Contact Sensors

Contact sensors detect when a link touches another object. Essential for gripper feedback and foot contact detection.

### Contact Sensor Definition

```xml
<link name="left_foot">
  <collision name="foot_collision">
    <geometry>
      <box><size>0.2 0.1 0.05</size></box>
    </geometry>
  </collision>

  <sensor name="foot_contact" type="contact">
    <contact>
      <collision>foot_collision</collision>
    </contact>
    <update_rate>100.0</update_rate>

    <plugin name="contact_plugin" filename="libgazebo_ros_bumper.so">
      <ros>
        <namespace>/robot</namespace>
        <remapping>~/out:=left_foot/contact</remapping>
      </ros>
      <frame_name>left_foot</frame_name>
    </plugin>
  </sensor>
</link>
```

### Contact Sensor Output

Publishes `gazebo_msgs/ContactsState` with:
- List of contacts
- Contact positions
- Contact forces and torques

<KeyTakeaways>
- Collision modeling involves defining geometry, friction, and restitution
- Friction coefficients determine slip behavior (higher = more grip)
- Gravity can be customized per-world for different planetary simulations
- Sensors require both a sensor definition and a ROS 2 plugin
- Gazebo sensors publish data to ROS 2 topics for processing
- Camera, IMU, and contact sensors are essential for humanoid robots
</KeyTakeaways>

## Next Steps

In the next chapter, we'll explore Unity for high-fidelity rendering and interactive scene creation. While Gazebo excels at physics, Unity provides photorealistic graphics for human-robot interaction scenarios.

<ExerciseBlock
  question="Create a Gazebo world with a simple robot model that includes a camera sensor and an IMU sensor. Launch the simulation, then use ROS 2 commands to visualize the camera feed and IMU data in real-time."
  hints={[
    { title: "Hint 1", content: "Start with a basic URDF model with a single link for the base and separate links for camera and IMU." },
    { title: "Hint 2", content: "Use the libgazebo_ros_camera.so plugin for the camera and libgazebo_ros_imu_sensor.so for the IMU." },
    { title: "Hint 3", content: "Use 'ros2 topic list' to find the published topics, then 'ros2 run rqt_image_view rqt_image_view' for the camera and 'ros2 topic echo' for IMU data." }
  ]}
  solution={
    <div>
      <p><strong>Sample Solution:</strong></p>
      <p><strong>Step 1: Create simple_robot.urdf</strong></p>
      <pre>{`<?xml version="1.0"?>
<robot name="simple_robot">

  <!-- Base link -->
  <link name="base_link">
    <visual>
      <geometry>
        <box size="0.5 0.5 0.2"/>
      </geometry>
      <material name="blue">
        <color rgba="0 0 1 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <box size="0.5 0.5 0.2"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="10"/>
      <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.1" iyz="0.0" izz="0.1"/>
    </inertial>
  </link>

  <!-- Camera link -->
  <link name="camera_link">
    <visual>
      <geometry>
        <box size="0.05 0.05 0.05"/>
      </geometry>
    </visual>
    <sensor name="camera" type="camera">
      <update_rate>30</update_rate>
      <camera>
        <horizontal_fov>1.047</horizontal_fov>
        <image>
          <width>640</width>
          <height>480</height>
        </image>
        <clip>
          <near>0.1</near>
          <far>100</far>
        </clip>
      </camera>
      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
        <frame_name>camera_link</frame_name>
      </plugin>
    </sensor>
  </link>

  <joint name="camera_joint" type="fixed">
    <parent link="base_link"/>
    <child link="camera_link"/>
    <origin xyz="0.25 0 0.1" rpy="0 0 0"/>
  </joint>

  <!-- IMU link -->
  <link name="imu_link">
    <sensor name="imu" type="imu">
      <always_on>true</always_on>
      <update_rate>100</update_rate>
      <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">
        <frame_name>imu_link</frame_name>
      </plugin>
    </sensor>
  </link>

  <joint name="imu_joint" type="fixed">
    <parent link="base_link"/>
    <child link="imu_link"/>
    <origin xyz="0 0 0" rpy="0 0 0"/>
  </joint>

</robot>`}</pre>
      <p><strong>Step 2: Launch Gazebo</strong></p>
      <pre>{`gazebo --verbose simple_robot.urdf`}</pre>
      <p><strong>Step 3: Visualize sensor data</strong></p>
      <pre>{`# In terminal 1: View camera feed
ros2 run rqt_image_view rqt_image_view

# In terminal 2: View IMU data
ros2 topic echo /imu

# In terminal 3: Plot IMU angular velocity
ros2 run rqt_plot rqt_plot /imu/angular_velocity`}</pre>
      <p>You should see the camera feed displaying the Gazebo world and IMU data streaming in the terminal.</p>
    </div>
  }
/>
