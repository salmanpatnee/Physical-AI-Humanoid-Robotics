---
title: "4. Nav2 for Humanoid Path Planning and Locomotion"
sidebar_position: 4
chapter_type: "tutorial"
learning_goals:
  - "Understand the Nav2 navigation stack architecture and components"
  - "Configure Nav2 for humanoid robot locomotion and bipedal constraints"
  - "Implement behavior trees for complex navigation tasks"
  - "Deploy end-to-end navigation from Isaac Sim simulation to Jetson hardware"
prerequisites:
  - "Completed Chapters 1-3 of this module"
  - "Understanding of ROS 2 topics, services, and actions"
  - "Familiarity with YAML configuration files"
key_takeaways:
  - "Nav2 is the ROS 2 navigation stack that handles path planning, obstacle avoidance, and behavior coordination"
  - "Humanoid robots require specialized configuration for bipedal stability and footstep planning"
  - "Behavior trees enable complex, hierarchical navigation behaviors beyond simple 'go to goal' commands"
  - "The full Physical AI pipeline connects: Perception (Isaac ROS) → Localization (VSLAM) → Planning (Nav2) → Control (Robot)"
---

# Nav2 for Humanoid Path Planning and Locomotion




## Completing the AI-Robot Brain

Over the past three chapters, you've built the foundation for an intelligent, autonomous robot:

1. **Chapter 1**: Understood advanced perception and synthetic data generation
2. **Chapter 2**: Created photorealistic simulation environments in Isaac Sim
3. **Chapter 3**: Deployed GPU-accelerated VSLAM for real-time localization

Now, we complete the **AI-Robot Brain** with **autonomous navigation**the ability for a robot to plan paths, avoid obstacles, and execute complex locomotion behaviors.

**Nav2** (Navigation2) is the ROS 2 navigation stack that provides:
- **Global path planning**: Find the shortest route from A to B
- **Local trajectory planning**: Avoid dynamic obstacles in real-time
- **Behavior coordination**: Manage complex navigation tasks (recovery behaviors, waiting, retrying)
- **Costmap generation**: Represent obstacles and free space for planning

<LearningGoals>
- Understand the Nav2 navigation stack architecture and components
- Configure Nav2 for humanoid robot locomotion and bipedal constraints
- Implement behavior trees for complex navigation tasks
- Deploy end-to-end navigation from Isaac Sim simulation to Jetson hardware
</LearningGoals>

<Prerequisites>
- Completed Chapters 1-3 of this module
- Understanding of ROS 2 topics, services, and actions
- Familiarity with YAML configuration files
</Prerequisites>

## The Navigation Problem

Consider the task: **"Robot, go to the kitchen and bring me a glass of water."**

This simple command requires the robot to:

1. **Understand the goal**: "Kitchen" is a semantic location, not just coordinates
2. **Plan a path**: Navigate through hallways, around furniture
3. **Avoid obstacles**: Don't collide with chairs, pets, people
4. **Handle failures**: What if the path is blocked? Replan.
5. **Execute locomotion**: Walk bipedally without falling
6. **Reach the target**: Stop at the correct position and orientation

Nav2 solves components 2-4 (planning, obstacle avoidance, failure handling). Component 5 (bipedal locomotion) requires specialized controllers that we'll configure in this chapter.

## Nav2 Architecture

Nav2 is a **modular** system composed of specialized servers:

```
┌─────────────────────────────────────────────────────────────────────┐
│                         Nav2 Architecture                           │
└─────────────────────────────────────────────────────────────────────┘

┌──────────────┐     ┌──────────────┐     ┌──────────────┐
│   BT Navigator│ ──> │  Planner     │ ──> │  Controller  │
│   (Behavior   │     │  Server      │     │  Server      │
│    Trees)     │     │  (Global     │     │  (Local      │
│               │     │   Path)      │     │   Trajectory)│
└───────┬───────┘     └───────┬──────┘     └───────┬──────┘
        │                     │                     │
        │                     ▼                     ▼
        │            ┌─────────────────┐   ┌──────────────┐
        │            │  Costmap 2D     │   │  Velocity    │
        │            │  (Obstacles)    │   │  Smoother    │
        │            └─────────────────┘   └──────────────┘
        │
        ▼
┌──────────────────────────────────────────────────────────┐
│              Recovery Behaviors                          │
│  • Spin recovery (rotate to find path)                   │
│  • Back up (reverse if stuck)                            │
│  • Wait (pause for dynamic obstacle to move)             │
└──────────────────────────────────────────────────────────┘
```

### Core Components

| Component | Function | Algorithm Examples |
|-----------|----------|-------------------|
| **BT Navigator** | Coordinates all navigation tasks using behavior trees | BehaviorTree.CPP |
| **Planner Server** | Computes global path from start to goal | Navfn, Smac Planner, Theta\* |
| **Controller Server** | Generates velocity commands to follow path | DWB, TEB, MPPI |
| **Costmap 2D** | Maintains obstacle map for planning | Static layer, Inflation layer, Voxel layer |
| **Recovery Server** | Executes recovery behaviors when stuck | Spin, BackUp, Wait |
| **Smoother Server** | Smooths paths for more natural motion | Simple Smoother, Savitzky-Golay |

## Installing Nav2

```bash
# On Ubuntu 22.04 with ROS 2 Humble
sudo apt update
sudo apt install -y ros-humble-navigation2 ros-humble-nav2-bringup

# Verify installation
ros2 pkg list | grep nav2
# Should show: nav2_bt_navigator, nav2_planner, nav2_controller, etc.
```

## Configuring Nav2 for a Humanoid Robot

Let's configure Nav2 for a simulated humanoid robot. We'll use parameters appropriate for bipedal locomotion.

### Understanding Humanoid Locomotion Constraints

Humanoid robots have unique constraints compared to wheeled robots:

| Constraint | Wheeled Robot | Humanoid Robot |
|------------|---------------|----------------|
| **Minimum turn radius** | Small (can spin in place) | Large (requires walking in arc) |
| **Acceleration** | High | Low (limited by balance) |
| **Lateral movement** | Holonomic (can strafe) | Non-holonomic (limited side-stepping) |
| **Stability** | Always stable | Must maintain ZMP (Zero Moment Point) |
| **Speed variation** | Continuous | Discrete gaits (walk, run) |

These constraints require careful tuning of Nav2 parameters.

### Nav2 Parameter File for Humanoids

```yaml
# File: humanoid_nav2_params.yaml

bt_navigator:
  ros__parameters:
    use_sim_time: True
    global_frame: map
    robot_base_frame: base_link
    odom_topic: /visual_slam/tracking/odometry
    bt_loop_duration: 10
    default_server_timeout: 20

    # Behavior tree (defines navigation logic)
    default_nav_to_pose_bt_xml: "navigate_to_pose_w_replanning_and_recovery.xml"

controller_server:
  ros__parameters:
    use_sim_time: True
    controller_frequency: 10.0  # Lower for humanoids (walking is slower)

    # Use Regulated Pure Pursuit controller (good for bipedal)
    controller_plugins: ["FollowPath"]

    FollowPath:
      plugin: "nav2_regulated_pure_pursuit_controller::RegulatedPurePursuitController"

      # Velocity limits (conservative for humanoids)
      desired_linear_vel: 0.3  # 0.3 m/s walking speed
      max_linear_accel: 0.3    # Slow acceleration for stability
      max_linear_decel: 0.4    # Can decelerate slightly faster

      # Rotational parameters
      max_angular_vel: 0.3     # 0.3 rad/s (~17 deg/s)
      max_angular_accel: 0.3   # Slow rotation for balance

      # Lookahead parameters
      lookahead_dist: 0.6      # Look 0.6m ahead
      min_lookahead_dist: 0.3  # Minimum lookahead
      max_lookahead_dist: 0.9  # Maximum lookahead

      # Path following
      use_velocity_scaled_lookahead_dist: true
      min_approach_linear_velocity: 0.05  # Slow down near goal
      approach_velocity_scaling_dist: 0.6

      # Collision checking
      use_collision_detection: true
      max_allowed_time_to_collision_up_to_carrot: 1.0

      # Footprint tolerance
      transform_tolerance: 0.2  # Humanoid pose estimation less precise

planner_server:
  ros__parameters:
    use_sim_time: True
    planner_plugins: ["GridBased"]

    GridBased:
      plugin: "nav2_smac_planner/SmacPlanner2D"  # Grid-based A*
      tolerance: 0.1       # Goal tolerance (meters)
      downsample_costmap: false
      allow_unknown: true  # Can plan through unexplored areas

      # Smoother (make paths less jagged)
      smoother:
        max_iterations: 1000
        w_smooth: 0.3
        w_data: 0.2
        tolerance: 1e-10

costmap_common_params:
  ros__parameters:
    use_sim_time: True

    # Robot footprint (humanoid standing upright)
    # Approximated as rectangle: 0.4m wide x 0.3m deep
    footprint: "[ [-0.15, -0.2], [-0.15, 0.2], [0.15, 0.2], [0.15, -0.2] ]"

    # Inflation (safety distance around obstacles)
    inflation_layer:
      plugin: "nav2_costmap_2d::InflationLayer"
      inflation_radius: 0.5  # 0.5m buffer around obstacles
      cost_scaling_factor: 3.0

global_costmap:
  global_costmap:
    ros__parameters:
      use_sim_time: True
      global_frame: map
      robot_base_frame: base_link
      update_frequency: 1.0  # Update 1 Hz (less frequent for global map)
      publish_frequency: 1.0
      resolution: 0.05  # 5cm grid cells

      # Map size (10m x 10m)
      width: 10
      height: 10

      plugins: ["static_layer", "obstacle_layer", "inflation_layer"]

      static_layer:
        plugin: "nav2_costmap_2d::StaticLayer"
        map_subscribe_transient_local: True

      obstacle_layer:
        plugin: "nav2_costmap_2d::ObstacleLayer"
        observation_sources: depth_camera

        depth_camera:
          topic: /camera/depth/image_raw
          data_type: "PointCloud2"
          clearing: True
          marking: True
          max_obstacle_height: 2.0  # Detect obstacles up to 2m
          min_obstacle_height: 0.1  # Ignore ground plane

local_costmap:
  local_costmap:
    ros__parameters:
      use_sim_time: True
      global_frame: odom
      robot_base_frame: base_link
      update_frequency: 5.0  # Update 5 Hz (more frequent for local obstacles)
      publish_frequency: 2.0
      resolution: 0.05

      # Local costmap size (4m x 4m around robot)
      width: 4
      height: 4
      rolling_window: true

      plugins: ["obstacle_layer", "inflation_layer"]

recoveries_server:
  ros__parameters:
    use_sim_time: True

    # Recovery behaviors for humanoids
    recovery_plugins: ["spin", "backup", "wait"]

    spin:
      plugin: "nav2_recoveries/Spin"
      simulate_ahead_time: 2.0
      max_rotational_vel: 0.3  # Slow spin for humanoid
      min_rotational_vel: 0.1
      rotational_acc_lim: 0.2

    backup:
      plugin: "nav2_recoveries/BackUp"
      simulate_ahead_time: 2.0
      backup_dist: 0.3      # Back up 30cm
      backup_speed: 0.1     # Slow reverse walking

    wait:
      plugin: "nav2_recoveries/Wait"
      wait_duration: 3.0    # Wait 3 seconds for obstacle to move
```

## Setting Up Navigation in Isaac Sim

Before deploying to hardware, let's test in simulation.

### Step 1: Create a Navigation Scene

```python
# File: nav_test_scene.py
from omni.isaac.kit import SimulationApp
simulation_app = SimulationApp({"headless": False})

from omni.isaac.core import World
from omni.isaac.core.utils.stage import add_reference_to_stage
from omni.isaac.core.utils.nucleus import get_assets_root_path
import numpy as np

# Create world
world = World()
world.scene.add_default_ground_plane()

# Add humanoid robot
assets_root_path = get_assets_root_path()
robot_usd = f"{assets_root_path}/Isaac/Robots/Humanoids/unitree_h1.usd"

add_reference_to_stage(usd_path=robot_usd, prim_path="/World/H1")

# Add obstacles (walls and furniture)
obstacle_positions = [
    (3.0, 0.0, 1.0, [2.0, 0.1, 2.0]),  # Wall
    (-3.0, 0.0, 1.0, [2.0, 0.1, 2.0]), # Wall
    (0.0, 2.0, 0.5, [1.0, 1.0, 1.0]),  # Box
]

from omni.isaac.core.objects import DynamicCuboid

for i, (x, y, z, scale) in enumerate(obstacle_positions):
    obstacle = DynamicCuboid(
        prim_path=f"/World/Obstacle_{i}",
        position=np.array([x, y, z]),
        scale=np.array(scale),
        color=np.array([0.5, 0.5, 0.5])
    )
    world.scene.add(obstacle)

# Add camera for VSLAM
add_reference_to_stage(
    usd_path=f"{assets_root_path}/Isaac/Sensors/Camera/stereo_camera.usd",
    prim_path="/World/H1/stereo_camera"
)

print("Navigation test scene created!")
world.reset()

# Keep simulation running
while simulation_app.is_running():
    world.step(render=True)

simulation_app.close()
```

### Step 2: Launch Nav2 with Isaac Sim

```xml
<!-- File: isaac_sim_nav2.launch.py -->
<launch>
  <!-- Start Isaac Sim (external process) -->
  <!-- Run nav_test_scene.py separately -->

  <!-- Isaac ROS VSLAM -->
  <node pkg="isaac_ros_visual_slam" exec="isaac_ros_visual_slam" name="visual_slam">
    <param name="enable_imu_fusion" value="true"/>
    <param name="map_frame" value="map"/>
    <param name="odom_frame" value="odom"/>
    <param name="base_frame" value="base_link"/>
  </node>

  <!-- Nav2 stack -->
  <include file="$(find-pkg-share nav2_bringup)/launch/bringup_launch.py">
    <arg name="params_file" value="$(find-pkg-share my_robot_nav)/config/humanoid_nav2_params.yaml"/>
    <arg name="use_sim_time" value="true"/>
  </include>

  <!-- RViz for visualization -->
  <node pkg="rviz2" exec="rviz2" name="rviz2">
    <arg name="-d" value="$(find-pkg-share nav2_bringup)/rviz/nav2_default_view.rviz"/>
  </node>
</launch>
```

### Step 3: Send Navigation Goals

```python
#!/usr/bin/env python3
# File: send_nav_goal.py

import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PoseStamped
from nav2_msgs.action import NavigateToPose
from rclpy.action import ActionClient

class NavGoalSender(Node):
    def __init__(self):
        super().__init__('nav_goal_sender')

        # Action client for Nav2
        self._action_client = ActionClient(
            self,
            NavigateToPose,
            'navigate_to_pose'
        )

    def send_goal(self, x, y, yaw):
        """Send navigation goal to Nav2"""

        # Wait for action server
        self.get_logger().info("Waiting for Nav2 action server...")
        self._action_client.wait_for_server()

        # Create goal message
        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()

        # Position
        goal_msg.pose.pose.position.x = x
        goal_msg.pose.pose.position.y = y
        goal_msg.pose.pose.position.z = 0.0

        # Orientation (convert yaw to quaternion)
        import math
        goal_msg.pose.pose.orientation.z = math.sin(yaw / 2)
        goal_msg.pose.pose.orientation.w = math.cos(yaw / 2)

        self.get_logger().info(f"Sending goal: ({x:.2f}, {y:.2f}, {math.degrees(yaw):.0f}°)")

        # Send goal
        send_goal_future = self._action_client.send_goal_async(
            goal_msg,
            feedback_callback=self.feedback_callback
        )
        send_goal_future.add_done_callback(self.goal_response_callback)

    def feedback_callback(self, feedback_msg):
        """Receive navigation progress updates"""
        feedback = feedback_msg.feedback
        distance_remaining = feedback.distance_remaining
        self.get_logger().info(
            f"Distance remaining: {distance_remaining:.2f}m",
            throttle_duration_sec=1.0
        )

    def goal_response_callback(self, future):
        """Handle goal acceptance/rejection"""
        goal_handle = future.result()

        if not goal_handle.accepted:
            self.get_logger().error("Goal rejected by Nav2!")
            return

        self.get_logger().info("Goal accepted by Nav2, navigating...")

        # Wait for result
        result_future = goal_handle.get_result_async()
        result_future.add_done_callback(self.result_callback)

    def result_callback(self, future):
        """Handle navigation completion"""
        result = future.result().result

        if result:
            self.get_logger().info("✓ Navigation succeeded!")
        else:
            self.get_logger().error("✗ Navigation failed!")

def main():
    rclpy.init()
    node = NavGoalSender()

    # Send goal: Go to (x=2.0m, y=1.0m, facing east)
    import math
    node.send_goal(x=2.0, y=1.0, yaw=0.0)  # 0 rad = east

    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**Usage**:
```bash
# Terminal 1: Launch Isaac Sim with robot scene
python3 nav_test_scene.py

# Terminal 2: Launch Nav2 + VSLAM
ros2 launch my_robot_nav isaac_sim_nav2.launch.py

# Terminal 3: Send navigation goal
python3 send_nav_goal.py
```

## Behavior Trees for Complex Navigation

Simple "go to goal" navigation is insufficient for real-world humanoid robots. **Behavior Trees** (BTs) enable hierarchical, reactive behaviors.

### Example: "Patrol and Charge" Behavior

A security robot that patrols waypoints and returns to charge when battery is low:

```xml
<!-- File: patrol_and_charge.xml -->
<root main_tree_to_execute="MainTree">
  <BehaviorTree ID="MainTree">
    <Sequence>
      <!-- Check battery level -->
      <Condition ID="BatteryOK" threshold="20.0"/>

      <!-- If battery OK, patrol -->
      <Fallback>
        <Sequence>
          <!-- Navigate to waypoint 1 -->
          <NavigateToPose goal="{waypoint1}"/>

          <!-- Navigate to waypoint 2 -->
          <NavigateToPose goal="{waypoint2}"/>

          <!-- Navigate to waypoint 3 -->
          <NavigateToPose goal="{waypoint3}"/>
        </Sequence>

        <!-- If patrol fails, retry -->
        <RetryUntilSuccessful num_attempts="3">
          <RecoveryNode/>
        </RetryUntilSuccessful>
      </Fallback>

      <!-- If battery low, go to charger -->
      <Fallback>
        <NavigateToPose goal="{charger_pose}"/>
        <ForceFailure/>  <!-- If can't reach charger, fail tree -->
      </Fallback>

      <!-- Dock with charger -->
      <DockAction/>
    </Sequence>
  </BehaviorTree>
</root>
```

**BT Nodes Explained**:
- **Sequence**: Execute children left-to-right, fail if any fails
- **Fallback**: Try children until one succeeds
- **Condition**: Check a boolean (e.g., battery > 20%)
- **RetryUntilSuccessful**: Repeat child up to N times
- **NavigateToPose**: Nav2 action to reach a goal

<KeyTakeaways>
- Nav2 is the ROS 2 navigation stack that handles path planning, obstacle avoidance, and behavior coordination
- Humanoid robots require specialized configuration for bipedal stability and footstep planning
- Behavior trees enable complex, hierarchical navigation behaviors beyond simple 'go to goal' commands
- The full Physical AI pipeline connects: Perception (Isaac ROS) → Localization (VSLAM) → Planning (Nav2) → Control (Robot)
</KeyTakeaways>

## Practical Exercise: Warehouse Navigation Task

<ExerciseBlock
  question="Deploy a complete navigation system for a humanoid robot in a simulated warehouse. The robot must: (1) Build a map of the warehouse using VSLAM, (2) Navigate autonomously to three delivery waypoints, (3) Return to a home position, (4) Handle obstacle avoidance. Provide the configuration files and launch sequence."
  hints={[
    {
      title: "Hint 1: Multi-Step Workflow",
      content: "Break this into phases: Phase 1 (Mapping) - manually control robot to build map, save map. Phase 2 (Navigation) - load saved map, send waypoint goals sequentially."
    },
    {
      title: "Hint 2: Map Saving",
      content: "Use Nav2's map saver: ros2 run nav2_map_server map_saver_cli -f warehouse_map. This saves map.pgm (image) and map.yaml (metadata)."
    },
    {
      title: "Hint 3: Waypoint Sequencing",
      content: "Create a Python node that sends goals sequentially. Wait for each goal to complete (result) before sending the next. Use NavigateToPose action."
    }
  ]}
  solution={
    <div>
      <p><strong>Solution: Warehouse Navigation System</strong></p>
      <p><strong>Part 1: Warehouse Scene (warehouse_scene.py)</strong></p>
      <pre>{`
# Create warehouse in Isaac Sim
from omni.isaac.kit import SimulationApp
simulation_app = SimulationApp({"headless": False})

from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid
from omni.isaac.core.utils.stage import add_reference_to_stage
import numpy as np

world = World()
world.scene.add_default_ground_plane()

# Add humanoid
add_reference_to_stage("/Isaac/Robots/Humanoids/unitree_h1.usd", "/World/Robot")

# Add warehouse shelves (grid layout)
for i in range(4):
    for j in range(2):
        shelf = DynamicCuboid(
            prim_path=f"/World/Shelf_{i}_{j}",
            position=np.array([i * 3.0, j * 4.0, 1.25]),
            scale=np.array([1.0, 0.3, 2.5]),
            color=np.array([0.5, 0.3, 0.1])
        )
        world.scene.add(shelf)

# Add stereo camera to robot
add_reference_to_stage("/Isaac/Sensors/Camera/stereo_camera.usd", "/World/Robot/camera")

print("Warehouse scene ready!")
world.reset()

while simulation_app.is_running():
    world.step(render=True)
      `}</pre>
      <p><strong>Part 2: Mapping Phase Launch (mapping.launch.py)</strong></p>
      <pre>{`
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        # Isaac ROS VSLAM
        Node(
            package='isaac_ros_visual_slam',
            executable='isaac_ros_visual_slam',
            parameters=[{
                'enable_imu_fusion': True,
                'map_frame': 'map',
            }]
        ),

        # Teleopkeyboard for manual control
        Node(
            package='teleop_twist_keyboard',
            executable='teleop_twist_keyboard',
            prefix='xterm -e',
            output='screen'
        ),

        # RViz
        Node(
            package='rviz2',
            executable='rviz2',
        ),
    ])
      `}</pre>
      <p><strong>Mapping Instructions:</strong></p>
      <pre>{`
# 1. Launch Isaac Sim
python3 warehouse_scene.py

# 2. Launch mapping
ros2 launch warehouse_nav mapping.launch.py

# 3. Drive robot around warehouse using keyboard
# Arrow keys to move, cover all areas

# 4. Save map when complete
ros2 run nav2_map_server map_saver_cli -f warehouse_map

# Output: warehouse_map.pgm, warehouse_map.yaml
      `}</pre>
      <p><strong>Part 3: Navigation Phase Config (warehouse_nav_params.yaml)</strong></p>
      <pre>{`
# Use humanoid parameters from main chapter, plus:

map_server:
  ros__parameters:
    yaml_filename: "warehouse_map.yaml"

amcl:
  ros__parameters:
    use_sim_time: True
    # AMCL localization parameters
    min_particles: 500
    max_particles: 2000
    initial_pose:
      x: 0.0
      y: 0.0
      yaw: 0.0
      `}</pre>
      <p><strong>Part 4: Navigation Phase Launch (navigation.launch.py)</strong></p>
      <pre>{`
from launch import LaunchDescription
from launch_ros.actions import Node
from launch.actions import IncludeLaunchDescription
from ament_index_python.packages import get_package_share_directory
import os

def generate_launch_description():
    nav2_bringup_dir = get_package_share_directory('nav2_bringup')

    return LaunchDescription([
        # Map server (load saved map)
        Node(
            package='nav2_map_server',
            executable='map_server',
            parameters=[{
                'yaml_filename': 'warehouse_map.yaml',
                'use_sim_time': True
            }]
        ),

        # Lifecycle manager (activate map server)
        Node(
            package='nav2_lifecycle_manager',
            executable='lifecycle_manager',
            parameters=[{
                'node_names': ['map_server'],
                'autostart': True
            }]
        ),

        # AMCL localization (uses VSLAM odometry)
        IncludeLaunchDescription(
            os.path.join(nav2_bringup_dir, 'launch', 'localization_launch.py'),
            launch_arguments={'use_sim_time': 'True'}.items()
        ),

        # Nav2 stack
        IncludeLaunchDescription(
            os.path.join(nav2_bringup_dir, 'launch', 'navigation_launch.py'),
            launch_arguments={
                'params_file': 'warehouse_nav_params.yaml',
                'use_sim_time': 'True'
            }.items()
        ),

        # Waypoint follower
        Node(
            package='warehouse_nav',
            executable='waypoint_follower.py'
        ),
    ])
      `}</pre>
      <p><strong>Part 5: Waypoint Follower (waypoint_follower.py)</strong></p>
      <pre>{`
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from nav2_msgs.action import NavigateToPose
from rclpy.action import ActionClient
from geometry_msgs.msg import PoseStamped
import math

class WaypointFollower(Node):
    def __init__(self):
        super().__init__('waypoint_follower')

        self._action_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')

        # Define waypoints (delivery locations + home)
        self.waypoints = [
            {'name': 'Delivery 1', 'x': 3.0, 'y': 2.0, 'yaw': 0.0},
            {'name': 'Delivery 2', 'x': 6.0, 'y': 4.0, 'yaw': math.pi/2},
            {'name': 'Delivery 3', 'x': 9.0, 'y': 2.0, 'yaw': math.pi},
            {'name': 'Home', 'x': 0.0, 'y': 0.0, 'yaw': 0.0},
        ]

        self.current_waypoint_index = 0

    def start_mission(self):
        """Start navigating through waypoints"""
        self.send_next_waypoint()

    def send_next_waypoint(self):
        """Send next waypoint goal"""
        if self.current_waypoint_index >= len(self.waypoints):
            self.get_logger().info("✓ Mission complete!")
            return

        wp = self.waypoints[self.current_waypoint_index]
        self.get_logger().info(f"Navigating to {wp['name']}...")

        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()
        goal_msg.pose.pose.position.x = wp['x']
        goal_msg.pose.pose.position.y = wp['y']
        goal_msg.pose.pose.orientation.z = math.sin(wp['yaw'] / 2)
        goal_msg.pose.pose.orientation.w = math.cos(wp['yaw'] / 2)

        send_future = self._action_client.send_goal_async(goal_msg)
        send_future.add_done_callback(self.goal_accepted_callback)

    def goal_accepted_callback(self, future):
        """Handle goal acceptance"""
        goal_handle = future.result()
        if not goal_handle.accepted:
            self.get_logger().error("Goal rejected!")
            return

        result_future = goal_handle.get_result_async()
        result_future.add_done_callback(self.goal_completed_callback)

    def goal_completed_callback(self, future):
        """Handle goal completion"""
        wp = self.waypoints[self.current_waypoint_index]
        result = future.result().result

        if result:
            self.get_logger().info(f"✓ Reached {wp['name']}")
            self.current_waypoint_index += 1
            self.send_next_waypoint()  # Continue to next
        else:
            self.get_logger().error(f"✗ Failed to reach {wp['name']}")

def main():
    rclpy.init()
    node = WaypointFollower()
    node._action_client.wait_for_server()
    node.start_mission()
    rclpy.spin(node)

if __name__ == '__main__':
    main()
      `}</pre>
      <p><strong>Deployment Instructions:</strong></p>
      <pre>{`
# 1. Launch Isaac Sim warehouse
python3 warehouse_scene.py

# 2. Launch navigation stack
ros2 launch warehouse_nav navigation.launch.py

# Robot will automatically navigate through all waypoints and return home

# Monitor progress in RViz:
# - Green line = planned global path
# - Robot icon moves through waypoints
# - Costmap shows obstacles (red = blocked)
      `}</pre>
      <p><strong>Key Features:</strong></p>
      <ul>
        <li><strong>Two-phase workflow</strong>: Separate mapping (manual) from navigation (autonomous)</li>
        <li><strong>Persistent map</strong>: Map is saved to disk and reused, avoiding remapping each run</li>
        <li><strong>Sequential waypoints</strong>: Robot waits for each goal to complete before proceeding</li>
        <li><strong>Automatic recovery</strong>: Nav2's recovery behaviors handle temporary blockages</li>
        <li><strong>Sim-to-real ready</strong>: Same code works in Isaac Sim and on real Jetson hardware</li>
      </ul>
    </div>
  }
/>

## From Simulation to Reality: Deploying to Jetson

The final step is deploying your tested navigation system to real hardware.

### Hardware Checklist

- ✅ NVIDIA Jetson Orin Nano with JetPack 6.0
- ✅ Intel RealSense D435i camera
- ✅ Humanoid robot with ROS 2 interface (or wheeled proxy)
- ✅ Battery and power management
- ✅ Wi-Fi for remote monitoring

### Deployment Steps

```bash
# On Jetson:

# 1. Copy your Nav2 configuration
scp -r warehouse_nav/ jetson@jetson-ip:~/ros2_ws/src/

# 2. Build workspace
cd ~/ros2_ws
colcon build --packages-select warehouse_nav

# 3. Launch on hardware
ros2 launch warehouse_nav navigation.launch.py use_sim_time:=False

# The same code runs on Jetson!
```

### Performance Monitoring

```python
# Monitor navigation performance
ros2 topic hz /cmd_vel  # Control frequency (should be ~10 Hz)
ros2 topic hz /visual_slam/tracking/odometry  # VSLAM rate (~30 Hz)

# Check Jetson resource usage
jetson_stats  # Monitor GPU, CPU, power
```

## Conclusion: The Complete AI-Robot Brain

Congratulations! You've now completed the full Physical AI perception and navigation pipeline:

```
┌────────────────────────────────────────────────────────────────┐
│                    The AI-Robot Brain                          │
└────────────────────────────────────────────────────────────────┘

  Cameras/Sensors  →  Isaac ROS  →  VSLAM  →  Nav2  →  Robot
                     (Perception)  (Localization) (Planning) (Action)

       │                  │            │          │         │
       │                  │            │          │         │
  Physical World    GPU-accelerated   Where    Path &    Motion
  (photons, IMU)    feature extraction am I?  obstacle  execution
                    object detection           avoidance
```

You've learned:
- ✅ Advanced perception concepts and synthetic data (Chapter 1)
- ✅ Photorealistic simulation in Isaac Sim (Chapter 2)
- ✅ GPU-accelerated VSLAM on Jetson (Chapter 3)
- ✅ Autonomous navigation with Nav2 (Chapter 4)

In Module 4 (Vision-Language-Action), you'll add the final layer: **natural language understanding**, enabling robots to receive voice commands like "Go to the kitchen and bring me water" and translate them into the navigation goals you've mastered here.


## Next Steps

Practice deploying these skills:
1. Build custom warehouse layouts in Isaac Sim
2. Test different Nav2 planners (TEB, MPPI) for different robot morphologies
3. Create complex behavior trees for multi-robot coordination
4. Optimize Nav2 parameters for your specific robot's dynamics

The future of Physical AI is in your hands!
